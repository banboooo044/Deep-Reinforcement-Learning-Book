{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モンテカルロ法\n",
    "方策勾配法(2_3)や方策反復法, 価値反復法(2_4_2) では遷移確率 $P$ が既知であることを前提としていた。\n",
    "\n",
    "確かに迷路ゲームでは, 前の状態と行動が決まれば次の状態が一意に決まるためこの手法で解けた。\n",
    "\n",
    "今回は, 遷移確率 $P$ がわからない例として, ブラックジャックゲームの学習を行う. \n",
    "\n",
    "## 1. ブラックジャックのルール\n",
    "ブラックシャックはカジノで定番のゲームで、以下のようなルール。\n",
    "\n",
    "1. ルールの概要\n",
    "  * トランプを使用する。\n",
    "  * トランプは無限デッキあると仮定する。（＝カードの出る確率は変化しない）\n",
    "  * Aは1もしくは11として扱う。\n",
    "  * 2〜10は数字通り扱う。\n",
    "  * J, Q, Kは10として扱う。\n",
    "  * カードの合計が21を越えず、出来るだけ21に近い方が勝ち。（同じなら引き分け）\n",
    "  \n",
    "2. プレイの流れ\n",
    "  * ユーザーにカードが2枚オープンで配られる。\n",
    "  * ディーラーにカードが1枚はオープン、もう1枚はクローズで配られる。\n",
    "  * プレイヤーは以下の行動が出来る。\n",
    "  * ヒット（カードをもう1枚引く）\n",
    "  * スタンド（カードを引くのを止める）\n",
    "  * カードの合計が21を越えたら、その時点でプレイヤーの負け。\n",
    "  * スタンドするか21を越えるまでは、何度でもヒット出来る。\n",
    "  *  プレイヤーがスタンドを選択したら、ディーラーは伏せていたカードをオープンにし、カードの合計が17以上になるまでカードを引く。\n",
    "  * カードの合計が21を越えたら、その時点でプレイヤーの勝ち。\n",
    "  * ディーラーのカードの合計が21以下の場合、カードの合計を比べる。\n",
    "  * カードの合計が21に近い方の勝ち。同じなら引き分け。\n",
    "\n",
    "## 2. マルコフ決定過程としてのブラックジャックゲーム\n",
    "1. 状態集合 $S$ <br>\n",
    "     $$ S = \\{ \\text{プレイヤーの状態} \\}  \\times \\{ \\text{ディーラーの状態}\\} \\cup \\{ \\text{Player勝ち}, \\text{Player負け}, \\text{引き分け} \\}$$\n",
    "     \n",
    "     * プレイヤーの状態 <br>\n",
    "         得点が11以下ならhitしても得点が21を超えない.  <br>\n",
    "         よって, プレイヤーの手持ちの得点は12以上の状態から始まるものとできる.  <br>\n",
    "         'A'は1と扱うこともできるし11と扱うこともできる. そこで, 11として扱う'A'が存在するかどうかで場合分け.\n",
    "         * ( 得点合計 12 ~ 21, 11点として扱う'A'が存在 )\n",
    "         * ( 得点合計 12 ~ 21, 11点として扱う'A'が存在しない )\n",
    "          \n",
    "     * ディーラーの状態 <br>\n",
    "       　　 1つのカードは見えていない. もう1つのカードは見えていて, [ 'A', 2 ~ 10 ] の場合がある.\n",
    "\n",
    "2. 行動集合 $A$ <br>\n",
    "    $$ A = \\{ \\text{hit}, \\text{stand} \\} $$\n",
    "    \n",
    "3. 報酬関数 $r$ <br>\n",
    "    $$ r(s, a, \\text{Player勝ち} ) = 1,\\ r(s, a, \\text{Player負け} ) = -1,\\ r(s, a, \\text{引き分け} ) = 0 \\ ( \\forall s \\in S,\\ \\forall a \\in A)$$\n",
    "    \n",
    "4. 遷移関数 $P$ <br>\n",
    "    hitかstandか行動を取った後にどの状態に行くかは確率的に変化する. この確率が未知とする.\n",
    "    \n",
    "5. 割引率 $ \\gamma $ <br>\n",
    "    $$\\gamma = 1.0 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:43:57.524421Z",
     "start_time": "2019-10-04T07:43:55.198005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Player (total : 12) [ 10, 2 ]\n",
      "Dealer (total : ??) [ 10, 8, ?? ]\n",
      "--------------------\n",
      "[h] : hit, [s] : stand \n",
      "h\n",
      "--------------------\n",
      "Player (total : 22) [ 10, 2, J ]\n",
      "Dealer (total : 18) [ 10, 8 ]\n",
      "--------------------\n",
      "Player Lose!\n"
     ]
    }
   ],
   "source": [
    "# ブラックジャックゲームで遊ぶ. \n",
    "# blackjackゲームのclassはblackjack.pyを参照.\n",
    "from blackjack import *\n",
    "game = BlackJack()\n",
    "while True:\n",
    "    game.show_status()\n",
    "    s = input(\"[h] : hit, [s] : stand \\n\")\n",
    "    if s == \"h\":\n",
    "        game.player_hit()\n",
    "    elif s == \"s\":\n",
    "        game.player_stand()\n",
    "    else:\n",
    "        print(\"Wrong Input\")\n",
    "        continue\n",
    "    if game.finish:\n",
    "        break\n",
    "\n",
    "game.show_status()\n",
    "result = game.result()\n",
    "if result > 0:\n",
    "    print(\"Player Win!\")\n",
    "elif result < 0:\n",
    "    print(\"Player Lose!\")\n",
    "else:\n",
    "    print(\"Draw.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モンテカルロ法の概要\n",
    "\n",
    "### 3.1 方策評価\n",
    "方策反復法や価値反復法では, 状態価値関数 $V^{\\pi}$ を求めることで方策 $\\pi$ の評価を行なっていた.\n",
    "    \n",
    "今回も同様に $V^{\\pi},\\ Q^{\\pi}$ を求めたい. しかし, そもそもベルマン方程式が連立方程式として解けない.\n",
    "\n",
    "さらに $P$ がわからない時 $Q^{\\pi}$ から $V^{\\pi}$ は求まるが, $V^{\\pi}$ から $Q^{\\pi}$ は求まらない. (参照 : 2_4_2 の 4. )\n",
    "\n",
    "なので, $Q^{\\pi}$ を定義に戻って求めたいと考える.\n",
    "$$ Q^{\\pi} (s,a) = \\mathbb{E}[ \\sum_{n=t}^{\\infty} \\gamma^{n-t} r_n  ; s_t = s,\\ a_t = a ] $$\n",
    "\n",
    "期待値は「理論上の平均」であることから, シミュレーションをして得られた収益の平均で期待値を推定する.\n",
    "$$ \\mathbb{E}[ \\sum_{n=t}^{\\infty} \\gamma^{n-t} r_n  ; s_t = s,\\ a_t = a ] \\simeq \\dfrac{1}{M} \\sum_{m=1}^M \\sum_{n=t}^{\\infty} \\gamma^{n-t} r_{m,n}  \\ (s_{m, t} = s,\\ a_{m,t} = a )$$\n",
    "\n",
    "> 大数の(強)法則 : 確率変数$X_1,\\ X_2,\\ \\dots, X_n$ が独立同分布に従っていれば, \n",
    "$$ P( \\lim_{n \\to \\infty} \\dfrac{1}{n} \\sum_{i=1}^n X_i = \\mu ) = 1 $$\n",
    "\n",
    "どのサンプルを採用するかによって2つの手法がある.\n",
    "* 初回訪問 : \n",
    "    1つのエピソード列中で, 初めて観測されたときの状態の収益だけを扱う.$ \\forall t' < t,\\  (s_{t'}, a_{t'}) \\neq (s,a) \\land (s_t, a_t) = (s,a) $ の時, $R_t = \\sum_{n = t}^{\\infty} \\gamma^{n - t} r_{n}$ をサンプルとして採用.\n",
    "* 逐次訪問 : 観測された全ての状態の収益をサンプルとして扱う. <br>\n",
    "    $ (s_t, a_t) = (s, a) $ の時, $R_t = \\sum_{n = t}^{\\infty} \\gamma^{n - t} r_{n}$ をサンプルとして採用.\n",
    "\n",
    "### 3.2 方策改善\n",
    "方策改善は, 2_4_2 と同じように greedy または $\\epsilon$-greedy で更新を行う. 再記述すると, \n",
    "* greedy : \n",
    "    $$ \\pi'(s, a) \\leftarrow \\begin{cases} 1 \\quad ( a = \\underset{a'\\in A}{\\operatorname{argmax}} Q^{\\pi}(s, a') ) \\\\ 0 \\quad ( otherwise ) \\end{cases} $$\n",
    "\n",
    "* $\\epsilon$-greedy : \n",
    "    $$ \\pi'(s, a) \\leftarrow \\begin{cases} 1 - (|A| - 1) \\epsilon \\quad &( a = \\underset{a'\\in A}{\\operatorname{argmax}} Q^{\\pi}(s, a') ) \\\\ \\epsilon & \\quad ( otherwise ) \\end{cases} $$\n",
    " \n",
    "### 3.3 乱数シュミレーションの問題点\n",
    "\n",
    "シュミレーションする時, 全ての状態が観測され, 各状態から様々な行動を取った結果の収益を平均化したい.\n",
    "しかし, 決定論的な方策を取っていると1つの状態からは同じ行動しか取らず, 学習がうまくいかない。<br>\n",
    "( greedyで方策改善すると決定論的な方策になる )\n",
    "\n",
    "これを解決する方法がいくつか考えられている.\n",
    "\n",
    "* 開始点探査の仮定をおき, 任意の状態行動対を開始点としてシミュレーションする. &rarr; モンテカルロES法\n",
    "開始点探査の仮定とは, 「任意の状態行動対 $(s, a) $ が出現する確率が0ではない」という仮定.\n",
    "方策が決定論的であっても任意の状態行動対 $(s, a) $ が生成される.\n",
    "\n",
    "* 方策を決定論的でないソフトな方策 $\\epsilon$-greedyに変更する. &rarr; 方策オン型モンテカルロ法\n",
    "\n",
    "* シュミレーション時の方策( 挙動方策 ) と本番用の方策 ( 推定方策 ) を別にする. &rarr; 方策オフ型モンテカルロ法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 モンテカルロ-ES法\n",
    "ここでは, 方策評価は初回訪問の方式で行う. <br>\n",
    "価値反復法のように $Q^{\\pi}$ の更新と $\\pi$ の更新を交互に行う.\n",
    "\n",
    "<img src=\"https://yoheitaonishi.com/wp-content/uploads/2018/10/es.png\" width=500 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:44:19.936742Z",
     "start_time": "2019-10-04T07:43:57.528069Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:22<00:00, 2235.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#%load_ext autoreload\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 状態集合の定義\n",
    "# S : 状態集合, S_to_num : 各状態に整数を割り振った対応表\n",
    "S_to_num = OrderedDict()\n",
    "state_num = 0\n",
    "for player_score in range(12, 22):\n",
    "    for ace_num in [ 0, 1 ]:\n",
    "        for dealer_card in range(1, 11):\n",
    "            S_to_num[ ( player_score, ace_num, dealer_card)] =  state_num\n",
    "            state_num += 1\n",
    "S_to_num[\"PlayerWin\"] = state_num\n",
    "S_to_num[\"PlayerLose\"] = state_num + 1\n",
    "S_to_num[\"Draw\"] = state_num + 2\n",
    "S = list(S_to_num.keys())\n",
    "\n",
    "# 行動集合 A, A_to_num : 各行動に整数を割り振った対応表\n",
    "A_to_num = { \"hit\" : 0, \"stand\" : 1}\n",
    "A = [ \"hit\" , \"stand\" ]\n",
    "\n",
    "# 報酬関数\n",
    "def reward(s1, a, s2):\n",
    "    if s2 ==\"PlayerWin\":\n",
    "        return 1\n",
    "    elif s2 == \"PlayerLose\":\n",
    "        return -1\n",
    "    elif s2 == \"Draw\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# 割引率\n",
    "gamma = 1.0\n",
    "\n",
    "# (s_0, a_0) から 方策piに従ってゲームが進行した時のエピソード列を返す.\n",
    "def generate_episode(pi, s_0, a_0):\n",
    "    game = BlackJack(s_0=s_0)\n",
    "    episode = [ [s_0, a_0] ]\n",
    "    \n",
    "    # 初期行動\n",
    "    if a_0 == \"hit\":\n",
    "        game.player_hit()\n",
    "    elif a_0 == \"stand\":\n",
    "        game.player_stand()\n",
    "    # ゲームの状態を取得.\n",
    "    s = game.state()\n",
    "    episode[-1].append(reward(s_0, a_0, s))\n",
    "    \n",
    "    while not game.finish:\n",
    "        # 次の行動を選択.\n",
    "        a = np.random.choice(A, p=pi[S_to_num[s] , :])\n",
    "        if a == \"hit\":\n",
    "            game.player_hit()\n",
    "        elif a == \"stand\":\n",
    "            game.player_stand()\n",
    "        # 次の状態を取得.\n",
    "        next_s = game.state()\n",
    "        # 報酬を取得.\n",
    "        r = reward(s, a, next_s)\n",
    "        # エピソード列に追加.\n",
    "        episode.append([ s, a, r ])\n",
    "        s = next_s\n",
    "    return episode\n",
    "\n",
    "# 与えられたpiを確率化する.\n",
    "def to_prob(pi):\n",
    "    n, m = pi.shape\n",
    "    for i in range(n):\n",
    "        pi[i] = pi[i, :] / np.sum(pi[i, :])\n",
    "    return pi\n",
    "\n",
    "def greedy(Q):\n",
    "    argmax = np.argmax(Q, axis=1)\n",
    "    new_pi = np.zeros_like(Q)\n",
    "    for s_raw in S:\n",
    "        s = S_to_num[s_raw]\n",
    "        new_pi[s , argmax[s]] = 1.0\n",
    "    return new_pi\n",
    "    \n",
    "# モンテカルロES法\n",
    "def Monte_Carlo_ES():\n",
    "    # Q, pi を適当に初期化.\n",
    "    Q = np.random.rand(  len(S), len(A) )\n",
    "    pi = to_prob(np.random.rand(  len(S), len(A) ))\n",
    "    \n",
    "    # 最後のゲーム終了状態は考える必要なし. ( 放置していても良いが, 念のため. )\n",
    "    Q[-3:, :] = 0\n",
    "    pi[-3:, :] = 0\n",
    "    \n",
    "    #  Returns[s][a] : [ 収益のリスト ]\n",
    "    Returns = [ [ [] for _ in range(len(A)) ] for _ in range(len(S)) ]\n",
    "    iterations = 50000\n",
    "    for roop in tqdm(range(iterations)):\n",
    "        # 初期状態\n",
    "        s_0 = np.random.choice(S)\n",
    "        # 終状態から実行しても意味がない\n",
    "        if s_0 == \"PlayerWin\" or s_0 == \"PlayerLose\" or s_0 == \"Draw\":\n",
    "            continue\n",
    "        # 初期行動\n",
    "        a_0 = np.random.choice(A)\n",
    "        \n",
    "        # エピソード生成.\n",
    "        episode = generate_episode(pi, s_0, a_0)\n",
    "        \n",
    "        # 既にエピソード列の中に状態行動対( s, a )が登場したかをもつ. Falseで初期化.\n",
    "        visited = np.zeros((len(S), len(A)))\n",
    "        \n",
    "        R_list = [0] * len (episode)\n",
    "        R_t = 0\n",
    "        # 各時刻における収益を計算\n",
    "        for i, (s_t, a_t, r_t) in enumerate(reversed(episode)):\n",
    "            s, a = S_to_num[s_t], A_to_num[a_t]\n",
    "            R_t += gamma*r_t\n",
    "            R_list[i] = R_t\n",
    "        R_list.reverse()\n",
    "\n",
    "        for i, (s_t, a_t, r_t) in enumerate(episode):\n",
    "            # 状態と行動を数字に変換\n",
    "            s, a = S_to_num[s_t], A_to_num[a_t]\n",
    "            # 収益の値.\n",
    "            R_t = R_list[i]\n",
    "            # 初回訪問の場合のみ扱う.\n",
    "            if not visited[s,a]:\n",
    "                # Returns[s][a]に(s,a)以降のエピソード列で得られる収益(報酬の和)を記録.\n",
    "                Returns[s][a].append(R_t)\n",
    "                # QはReturns[s][a]の平均で更新\n",
    "                # ※ len(Returns[s][a]) の値を記録する配列 Returns_Length[s][a] を定義すると, \n",
    "                # ※ Q[s][a] <- ( Q[s][a] * (Returns_Length[s][a] - 1) + R_t ) / Returns_Length[s][a] と更新が可能.\n",
    "                Q[s, a] = sum(Returns[s][a]) / len(Returns[s][a])\n",
    "                visited[s, a] = True\n",
    "        # piの更新.\n",
    "        pi = greedy(Q)\n",
    "            \n",
    "    return Q, pi, Returns\n",
    "  \n",
    "Q, pi_es, Returns = Monte_Carlo_ES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:44:19.956133Z",
     "start_time": "2019-10-04T07:44:19.939261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State : (12, 0, 1), best-pi : hit\n",
      "State : (12, 0, 2), best-pi : hit\n",
      "State : (12, 0, 3), best-pi : hit\n",
      "State : (12, 0, 4), best-pi : stand\n",
      "State : (12, 0, 5), best-pi : hit\n",
      "State : (12, 0, 6), best-pi : stand\n",
      "State : (12, 0, 7), best-pi : hit\n",
      "State : (12, 0, 8), best-pi : hit\n",
      "State : (12, 0, 9), best-pi : hit\n",
      "State : (12, 0, 10), best-pi : hit\n",
      "State : (12, 1, 1), best-pi : hit\n",
      "State : (12, 1, 2), best-pi : hit\n",
      "State : (12, 1, 3), best-pi : hit\n",
      "State : (12, 1, 4), best-pi : hit\n",
      "State : (12, 1, 5), best-pi : hit\n",
      "State : (12, 1, 6), best-pi : hit\n",
      "State : (12, 1, 7), best-pi : hit\n",
      "State : (12, 1, 8), best-pi : hit\n",
      "State : (12, 1, 9), best-pi : hit\n",
      "State : (12, 1, 10), best-pi : hit\n",
      "State : (13, 0, 1), best-pi : hit\n",
      "State : (13, 0, 2), best-pi : hit\n",
      "State : (13, 0, 3), best-pi : hit\n",
      "State : (13, 0, 4), best-pi : hit\n",
      "State : (13, 0, 5), best-pi : hit\n",
      "State : (13, 0, 6), best-pi : stand\n",
      "State : (13, 0, 7), best-pi : hit\n",
      "State : (13, 0, 8), best-pi : hit\n",
      "State : (13, 0, 9), best-pi : hit\n",
      "State : (13, 0, 10), best-pi : hit\n",
      "State : (13, 1, 1), best-pi : hit\n",
      "State : (13, 1, 2), best-pi : hit\n",
      "State : (13, 1, 3), best-pi : hit\n",
      "State : (13, 1, 4), best-pi : hit\n",
      "State : (13, 1, 5), best-pi : hit\n",
      "State : (13, 1, 6), best-pi : hit\n",
      "State : (13, 1, 7), best-pi : hit\n",
      "State : (13, 1, 8), best-pi : hit\n",
      "State : (13, 1, 9), best-pi : hit\n",
      "State : (13, 1, 10), best-pi : hit\n",
      "State : (14, 0, 1), best-pi : hit\n",
      "State : (14, 0, 2), best-pi : stand\n",
      "State : (14, 0, 3), best-pi : hit\n",
      "State : (14, 0, 4), best-pi : stand\n",
      "State : (14, 0, 5), best-pi : stand\n",
      "State : (14, 0, 6), best-pi : stand\n",
      "State : (14, 0, 7), best-pi : stand\n",
      "State : (14, 0, 8), best-pi : hit\n",
      "State : (14, 0, 9), best-pi : hit\n",
      "State : (14, 0, 10), best-pi : hit\n",
      "State : (14, 1, 1), best-pi : hit\n",
      "State : (14, 1, 2), best-pi : hit\n",
      "State : (14, 1, 3), best-pi : hit\n",
      "State : (14, 1, 4), best-pi : hit\n",
      "State : (14, 1, 5), best-pi : hit\n",
      "State : (14, 1, 6), best-pi : hit\n",
      "State : (14, 1, 7), best-pi : hit\n",
      "State : (14, 1, 8), best-pi : hit\n",
      "State : (14, 1, 9), best-pi : hit\n",
      "State : (14, 1, 10), best-pi : hit\n",
      "State : (15, 0, 1), best-pi : hit\n",
      "State : (15, 0, 2), best-pi : stand\n",
      "State : (15, 0, 3), best-pi : stand\n",
      "State : (15, 0, 4), best-pi : stand\n",
      "State : (15, 0, 5), best-pi : stand\n",
      "State : (15, 0, 6), best-pi : stand\n",
      "State : (15, 0, 7), best-pi : hit\n",
      "State : (15, 0, 8), best-pi : hit\n",
      "State : (15, 0, 9), best-pi : stand\n",
      "State : (15, 0, 10), best-pi : hit\n",
      "State : (15, 1, 1), best-pi : hit\n",
      "State : (15, 1, 2), best-pi : hit\n",
      "State : (15, 1, 3), best-pi : hit\n",
      "State : (15, 1, 4), best-pi : hit\n",
      "State : (15, 1, 5), best-pi : hit\n",
      "State : (15, 1, 6), best-pi : hit\n",
      "State : (15, 1, 7), best-pi : hit\n",
      "State : (15, 1, 8), best-pi : hit\n",
      "State : (15, 1, 9), best-pi : hit\n",
      "State : (15, 1, 10), best-pi : hit\n",
      "State : (16, 0, 1), best-pi : hit\n",
      "State : (16, 0, 2), best-pi : stand\n",
      "State : (16, 0, 3), best-pi : stand\n",
      "State : (16, 0, 4), best-pi : stand\n",
      "State : (16, 0, 5), best-pi : stand\n",
      "State : (16, 0, 6), best-pi : stand\n",
      "State : (16, 0, 7), best-pi : stand\n",
      "State : (16, 0, 8), best-pi : stand\n",
      "State : (16, 0, 9), best-pi : stand\n",
      "State : (16, 0, 10), best-pi : stand\n",
      "State : (16, 1, 1), best-pi : hit\n",
      "State : (16, 1, 2), best-pi : hit\n",
      "State : (16, 1, 3), best-pi : hit\n",
      "State : (16, 1, 4), best-pi : hit\n",
      "State : (16, 1, 5), best-pi : hit\n",
      "State : (16, 1, 6), best-pi : hit\n",
      "State : (16, 1, 7), best-pi : hit\n",
      "State : (16, 1, 8), best-pi : hit\n",
      "State : (16, 1, 9), best-pi : hit\n",
      "State : (16, 1, 10), best-pi : hit\n",
      "State : (17, 0, 1), best-pi : stand\n",
      "State : (17, 0, 2), best-pi : stand\n",
      "State : (17, 0, 3), best-pi : stand\n",
      "State : (17, 0, 4), best-pi : stand\n",
      "State : (17, 0, 5), best-pi : stand\n",
      "State : (17, 0, 6), best-pi : stand\n",
      "State : (17, 0, 7), best-pi : stand\n",
      "State : (17, 0, 8), best-pi : stand\n",
      "State : (17, 0, 9), best-pi : stand\n",
      "State : (17, 0, 10), best-pi : stand\n",
      "State : (17, 1, 1), best-pi : hit\n",
      "State : (17, 1, 2), best-pi : hit\n",
      "State : (17, 1, 3), best-pi : hit\n",
      "State : (17, 1, 4), best-pi : stand\n",
      "State : (17, 1, 5), best-pi : hit\n",
      "State : (17, 1, 6), best-pi : hit\n",
      "State : (17, 1, 7), best-pi : hit\n",
      "State : (17, 1, 8), best-pi : hit\n",
      "State : (17, 1, 9), best-pi : hit\n",
      "State : (17, 1, 10), best-pi : hit\n",
      "State : (18, 0, 1), best-pi : stand\n",
      "State : (18, 0, 2), best-pi : stand\n",
      "State : (18, 0, 3), best-pi : stand\n",
      "State : (18, 0, 4), best-pi : stand\n",
      "State : (18, 0, 5), best-pi : stand\n",
      "State : (18, 0, 6), best-pi : stand\n",
      "State : (18, 0, 7), best-pi : stand\n",
      "State : (18, 0, 8), best-pi : stand\n",
      "State : (18, 0, 9), best-pi : stand\n",
      "State : (18, 0, 10), best-pi : stand\n",
      "State : (18, 1, 1), best-pi : hit\n",
      "State : (18, 1, 2), best-pi : stand\n",
      "State : (18, 1, 3), best-pi : stand\n",
      "State : (18, 1, 4), best-pi : stand\n",
      "State : (18, 1, 5), best-pi : stand\n",
      "State : (18, 1, 6), best-pi : stand\n",
      "State : (18, 1, 7), best-pi : stand\n",
      "State : (18, 1, 8), best-pi : stand\n",
      "State : (18, 1, 9), best-pi : hit\n",
      "State : (18, 1, 10), best-pi : stand\n",
      "State : (19, 0, 1), best-pi : stand\n",
      "State : (19, 0, 2), best-pi : stand\n",
      "State : (19, 0, 3), best-pi : stand\n",
      "State : (19, 0, 4), best-pi : stand\n",
      "State : (19, 0, 5), best-pi : stand\n",
      "State : (19, 0, 6), best-pi : stand\n",
      "State : (19, 0, 7), best-pi : stand\n",
      "State : (19, 0, 8), best-pi : stand\n",
      "State : (19, 0, 9), best-pi : stand\n",
      "State : (19, 0, 10), best-pi : stand\n",
      "State : (19, 1, 1), best-pi : stand\n",
      "State : (19, 1, 2), best-pi : stand\n",
      "State : (19, 1, 3), best-pi : stand\n",
      "State : (19, 1, 4), best-pi : stand\n",
      "State : (19, 1, 5), best-pi : stand\n",
      "State : (19, 1, 6), best-pi : stand\n",
      "State : (19, 1, 7), best-pi : stand\n",
      "State : (19, 1, 8), best-pi : stand\n",
      "State : (19, 1, 9), best-pi : stand\n",
      "State : (19, 1, 10), best-pi : stand\n",
      "State : (20, 0, 1), best-pi : stand\n",
      "State : (20, 0, 2), best-pi : stand\n",
      "State : (20, 0, 3), best-pi : stand\n",
      "State : (20, 0, 4), best-pi : stand\n",
      "State : (20, 0, 5), best-pi : stand\n",
      "State : (20, 0, 6), best-pi : stand\n",
      "State : (20, 0, 7), best-pi : stand\n",
      "State : (20, 0, 8), best-pi : stand\n",
      "State : (20, 0, 9), best-pi : stand\n",
      "State : (20, 0, 10), best-pi : stand\n",
      "State : (20, 1, 1), best-pi : stand\n",
      "State : (20, 1, 2), best-pi : stand\n",
      "State : (20, 1, 3), best-pi : stand\n",
      "State : (20, 1, 4), best-pi : stand\n",
      "State : (20, 1, 5), best-pi : stand\n",
      "State : (20, 1, 6), best-pi : stand\n",
      "State : (20, 1, 7), best-pi : stand\n",
      "State : (20, 1, 8), best-pi : stand\n",
      "State : (20, 1, 9), best-pi : stand\n",
      "State : (20, 1, 10), best-pi : stand\n",
      "State : (21, 0, 1), best-pi : stand\n",
      "State : (21, 0, 2), best-pi : stand\n",
      "State : (21, 0, 3), best-pi : stand\n",
      "State : (21, 0, 4), best-pi : stand\n",
      "State : (21, 0, 5), best-pi : stand\n",
      "State : (21, 0, 6), best-pi : stand\n",
      "State : (21, 0, 7), best-pi : stand\n",
      "State : (21, 0, 8), best-pi : stand\n",
      "State : (21, 0, 9), best-pi : stand\n",
      "State : (21, 0, 10), best-pi : stand\n",
      "State : (21, 1, 1), best-pi : stand\n",
      "State : (21, 1, 2), best-pi : stand\n",
      "State : (21, 1, 3), best-pi : stand\n",
      "State : (21, 1, 4), best-pi : stand\n",
      "State : (21, 1, 5), best-pi : stand\n",
      "State : (21, 1, 6), best-pi : stand\n",
      "State : (21, 1, 7), best-pi : stand\n",
      "State : (21, 1, 8), best-pi : stand\n",
      "State : (21, 1, 9), best-pi : stand\n",
      "State : (21, 1, 10), best-pi : stand\n"
     ]
    }
   ],
   "source": [
    "# 最適方策の表示\n",
    "def show_policy(S, pi):\n",
    "    for s in S:\n",
    "        if s == \"PlayerWin\" or s == \"PlayerLose\" or s == \"Draw\":\n",
    "            continue\n",
    "        s_n = S_to_num[s]\n",
    "        if pi[s_n, 0] < pi[s_n, 1]:\n",
    "            print(\"State : {0}, best-pi : {1}\".format(s,  \"stand\" ))\n",
    "        if pi[s_n, 0] >= pi[s_n, 1]:\n",
    "            print(\"State : {0}, best-pi : {1}\".format(s, \"hit\"))\n",
    "            \n",
    "show_policy(S, pi_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 方策オン型モンテカルロ法\n",
    "\n",
    "方策を確率的なソフト方策 $\\epsilon$-greedyに変更することで, 任意の状態行動対 $(s, a)$ が生成される.\n",
    "\n",
    "<img src=\"https://yoheitaonishi.com/wp-content/uploads/2018/10/e%E3%82%BD%E3%83%95%E3%83%88%E6%96%B9%E7%AD%96.png\" width=500>\n",
    "\n",
    "#### [余談] greedy と $\\epsilon$-greedy の関係\n",
    "ここでは「任意の状態行動対 $(s, a)$ が生成する」目的で $\\epsilon$-greedy を導入したが, 2_4_2 で述べた方策反復法や価値反復法の方策改善アルゴリズムとして, greedy の代わりに $\\epsilon$-greedy を用いても良い.\n",
    "\n",
    "greedyなアルゴリズムでは, 初期値によっては局所的最適解に陥る.\n",
    "<img src=\"https://www.ffri.jp/assets/images/blog/201306/pic1.png\" width=500>\n",
    "\n",
    "そこで, 確率的なゆらぎを導入して, 大域的最適解を求める.\n",
    "\n",
    "学習が進むにつれて $\\epsilon$ の値 ( 図ではTemprature ) を小さくしていく.\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d5/Hill_Climbing_with_Simulated_Annealing.gif\" width=500>\n",
    "\n",
    "最適化において, greedyなアルゴリズムを確率的なアルゴリズムに拡張することはよくある.\n",
    "- (組み合わせ最適化) 山登り法 &rarr; 焼きなまし法(アニーリング)\n",
    "- (連続最適化) 最急降下法 &rarr; 確率的勾配法\n",
    "\n",
    "この拡張のメリットは, 大域的最適解が初期値によらず発見可能.デメリットは, 収束に時間がかかること.\n",
    "\n",
    "ちなみに凸関数( 山/谷が1つ )であれば局所的最適解と大域的最適解が一致するので特有の最適化手法があって速い. ([凸最適化](https://myenigma.hatenablog.com/entry/2016/11/01/135447))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:44:39.800819Z",
     "start_time": "2019-10-04T07:44:19.958864Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:19<00:00, 2524.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy(Q, epsilon):\n",
    "    # argmaxに 1 - epsilon + epsilon / |A| , それ以外に epsilon / |A|\n",
    "    argmax = np.argmax(Q, axis=1)\n",
    "    new_pi = np.ones_like(Q) * (epsilon / len(A))\n",
    "    for s_raw in S:\n",
    "        s = S_to_num[s_raw]\n",
    "        new_pi[s , argmax[s]] += 1 - epsilon\n",
    "    return new_pi\n",
    "\n",
    "# 初回訪問方策オン型モンテカルロ法, 本質的には, ES法からpiの更新部分が変更されているだけ.\n",
    "def on_policy_Monte_Carlo(epsilon = 0.3):\n",
    "    # Q, pi を適当に初期化.\n",
    "    Q = np.random.rand(  len(S), len(A) )\n",
    "    pi = to_prob(np.random.rand(  len(S), len(A) ))\n",
    "    \n",
    "    # 最後のゲーム終了状態は考える必要なし. ( 放置していても良いが, 念のため. )\n",
    "    Q[-3:, :] = 0\n",
    "    pi[-3:, :] = 0\n",
    "    # Returnsリストの長さを持つことにする.\n",
    "    Returns_Length = [ [0]*len(A) for _ in range(len(S)) ]\n",
    "    iterations = 50000\n",
    "    for roop in tqdm(range(iterations)):\n",
    "        # 初期状態\n",
    "        s_0 = np.random.choice(S)\n",
    "        # 終状態から実行しても意味がない\n",
    "        if s_0 == \"PlayerWin\" or s_0 == \"PlayerLose\" or s_0 == \"Draw\":\n",
    "            continue\n",
    "        # 初期行動\n",
    "        a_0 = np.random.choice(A)\n",
    "        \n",
    "        # エピソード生成.\n",
    "        episode = generate_episode(pi, s_0, a_0)\n",
    "        \n",
    "        # 既にエピソード列の中に状態行動対( s, a )が登場したかをもつ. Falseで初期化.\n",
    "        visited = np.zeros((len(S), len(A)))\n",
    "        \n",
    "        R_list = [0] * len (episode)\n",
    "        R_t = 0\n",
    "        # 各時刻における収益を計算\n",
    "        for i, (s_t, a_t, r_t) in enumerate(reversed(episode)):\n",
    "            s, a = S_to_num[s_t], A_to_num[a_t]\n",
    "            R_t += gamma*r_t\n",
    "            R_list[i] = R_t\n",
    "        R_list.reverse()\n",
    "    \n",
    "        for i, (s_t, a_t, r_t) in enumerate(episode):\n",
    "            # 状態と行動を数字に変換\n",
    "            s, a = S_to_num[s_t], A_to_num[a_t]\n",
    "            # 収益の値.\n",
    "            R_t = R_list[i]\n",
    "            # 初回訪問の場合のみ扱う.\n",
    "            if not visited[s,a]:\n",
    "                Returns_Length[s][a] += 1\n",
    "                # この式と等価: \n",
    "                # Q[s,a] = (Q[s,a] * (Returns_Length[s][a] - 1) + R_t) / Returns_Length[s][a]\n",
    "                Q[s,a] = Q[s,a] + (R_t - Q[s,a]) / (Returns_Length[s][a])\n",
    "                visited[s, a] = True\n",
    "        # piの更新. \n",
    "        pi = epsilon_greedy(Q, epsilon)\n",
    "        # epsilonを小さく\n",
    "        epsilon /= 2\n",
    "        \n",
    "    return Q, pi, Returns\n",
    "                \n",
    "Q, pi_on, Returns = on_policy_Monte_Carlo(epsilon=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:44:39.823351Z",
     "start_time": "2019-10-04T07:44:39.808823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State : (12, 0, 1), best-pi : hit\n",
      "State : (12, 0, 2), best-pi : stand\n",
      "State : (12, 0, 3), best-pi : hit\n",
      "State : (12, 0, 4), best-pi : hit\n",
      "State : (12, 0, 5), best-pi : stand\n",
      "State : (12, 0, 6), best-pi : stand\n",
      "State : (12, 0, 7), best-pi : hit\n",
      "State : (12, 0, 8), best-pi : hit\n",
      "State : (12, 0, 9), best-pi : hit\n",
      "State : (12, 0, 10), best-pi : hit\n",
      "State : (12, 1, 1), best-pi : hit\n",
      "State : (12, 1, 2), best-pi : hit\n",
      "State : (12, 1, 3), best-pi : hit\n",
      "State : (12, 1, 4), best-pi : hit\n",
      "State : (12, 1, 5), best-pi : hit\n",
      "State : (12, 1, 6), best-pi : stand\n",
      "State : (12, 1, 7), best-pi : hit\n",
      "State : (12, 1, 8), best-pi : hit\n",
      "State : (12, 1, 9), best-pi : hit\n",
      "State : (12, 1, 10), best-pi : hit\n",
      "State : (13, 0, 1), best-pi : hit\n",
      "State : (13, 0, 2), best-pi : hit\n",
      "State : (13, 0, 3), best-pi : hit\n",
      "State : (13, 0, 4), best-pi : stand\n",
      "State : (13, 0, 5), best-pi : stand\n",
      "State : (13, 0, 6), best-pi : hit\n",
      "State : (13, 0, 7), best-pi : hit\n",
      "State : (13, 0, 8), best-pi : hit\n",
      "State : (13, 0, 9), best-pi : stand\n",
      "State : (13, 0, 10), best-pi : hit\n",
      "State : (13, 1, 1), best-pi : hit\n",
      "State : (13, 1, 2), best-pi : hit\n",
      "State : (13, 1, 3), best-pi : hit\n",
      "State : (13, 1, 4), best-pi : hit\n",
      "State : (13, 1, 5), best-pi : hit\n",
      "State : (13, 1, 6), best-pi : hit\n",
      "State : (13, 1, 7), best-pi : hit\n",
      "State : (13, 1, 8), best-pi : hit\n",
      "State : (13, 1, 9), best-pi : hit\n",
      "State : (13, 1, 10), best-pi : hit\n",
      "State : (14, 0, 1), best-pi : hit\n",
      "State : (14, 0, 2), best-pi : stand\n",
      "State : (14, 0, 3), best-pi : stand\n",
      "State : (14, 0, 4), best-pi : stand\n",
      "State : (14, 0, 5), best-pi : stand\n",
      "State : (14, 0, 6), best-pi : hit\n",
      "State : (14, 0, 7), best-pi : hit\n",
      "State : (14, 0, 8), best-pi : hit\n",
      "State : (14, 0, 9), best-pi : hit\n",
      "State : (14, 0, 10), best-pi : stand\n",
      "State : (14, 1, 1), best-pi : hit\n",
      "State : (14, 1, 2), best-pi : hit\n",
      "State : (14, 1, 3), best-pi : hit\n",
      "State : (14, 1, 4), best-pi : hit\n",
      "State : (14, 1, 5), best-pi : hit\n",
      "State : (14, 1, 6), best-pi : hit\n",
      "State : (14, 1, 7), best-pi : hit\n",
      "State : (14, 1, 8), best-pi : hit\n",
      "State : (14, 1, 9), best-pi : hit\n",
      "State : (14, 1, 10), best-pi : hit\n",
      "State : (15, 0, 1), best-pi : hit\n",
      "State : (15, 0, 2), best-pi : stand\n",
      "State : (15, 0, 3), best-pi : stand\n",
      "State : (15, 0, 4), best-pi : stand\n",
      "State : (15, 0, 5), best-pi : stand\n",
      "State : (15, 0, 6), best-pi : stand\n",
      "State : (15, 0, 7), best-pi : hit\n",
      "State : (15, 0, 8), best-pi : stand\n",
      "State : (15, 0, 9), best-pi : stand\n",
      "State : (15, 0, 10), best-pi : stand\n",
      "State : (15, 1, 1), best-pi : hit\n",
      "State : (15, 1, 2), best-pi : hit\n",
      "State : (15, 1, 3), best-pi : hit\n",
      "State : (15, 1, 4), best-pi : hit\n",
      "State : (15, 1, 5), best-pi : hit\n",
      "State : (15, 1, 6), best-pi : hit\n",
      "State : (15, 1, 7), best-pi : hit\n",
      "State : (15, 1, 8), best-pi : hit\n",
      "State : (15, 1, 9), best-pi : hit\n",
      "State : (15, 1, 10), best-pi : hit\n",
      "State : (16, 0, 1), best-pi : hit\n",
      "State : (16, 0, 2), best-pi : stand\n",
      "State : (16, 0, 3), best-pi : stand\n",
      "State : (16, 0, 4), best-pi : stand\n",
      "State : (16, 0, 5), best-pi : stand\n",
      "State : (16, 0, 6), best-pi : stand\n",
      "State : (16, 0, 7), best-pi : hit\n",
      "State : (16, 0, 8), best-pi : stand\n",
      "State : (16, 0, 9), best-pi : stand\n",
      "State : (16, 0, 10), best-pi : hit\n",
      "State : (16, 1, 1), best-pi : hit\n",
      "State : (16, 1, 2), best-pi : hit\n",
      "State : (16, 1, 3), best-pi : hit\n",
      "State : (16, 1, 4), best-pi : hit\n",
      "State : (16, 1, 5), best-pi : hit\n",
      "State : (16, 1, 6), best-pi : hit\n",
      "State : (16, 1, 7), best-pi : hit\n",
      "State : (16, 1, 8), best-pi : hit\n",
      "State : (16, 1, 9), best-pi : hit\n",
      "State : (16, 1, 10), best-pi : hit\n",
      "State : (17, 0, 1), best-pi : stand\n",
      "State : (17, 0, 2), best-pi : stand\n",
      "State : (17, 0, 3), best-pi : stand\n",
      "State : (17, 0, 4), best-pi : stand\n",
      "State : (17, 0, 5), best-pi : stand\n",
      "State : (17, 0, 6), best-pi : stand\n",
      "State : (17, 0, 7), best-pi : stand\n",
      "State : (17, 0, 8), best-pi : stand\n",
      "State : (17, 0, 9), best-pi : stand\n",
      "State : (17, 0, 10), best-pi : stand\n",
      "State : (17, 1, 1), best-pi : hit\n",
      "State : (17, 1, 2), best-pi : hit\n",
      "State : (17, 1, 3), best-pi : hit\n",
      "State : (17, 1, 4), best-pi : hit\n",
      "State : (17, 1, 5), best-pi : hit\n",
      "State : (17, 1, 6), best-pi : stand\n",
      "State : (17, 1, 7), best-pi : hit\n",
      "State : (17, 1, 8), best-pi : hit\n",
      "State : (17, 1, 9), best-pi : hit\n",
      "State : (17, 1, 10), best-pi : hit\n",
      "State : (18, 0, 1), best-pi : stand\n",
      "State : (18, 0, 2), best-pi : stand\n",
      "State : (18, 0, 3), best-pi : stand\n",
      "State : (18, 0, 4), best-pi : stand\n",
      "State : (18, 0, 5), best-pi : stand\n",
      "State : (18, 0, 6), best-pi : stand\n",
      "State : (18, 0, 7), best-pi : stand\n",
      "State : (18, 0, 8), best-pi : stand\n",
      "State : (18, 0, 9), best-pi : stand\n",
      "State : (18, 0, 10), best-pi : stand\n",
      "State : (18, 1, 1), best-pi : hit\n",
      "State : (18, 1, 2), best-pi : stand\n",
      "State : (18, 1, 3), best-pi : stand\n",
      "State : (18, 1, 4), best-pi : stand\n",
      "State : (18, 1, 5), best-pi : hit\n",
      "State : (18, 1, 6), best-pi : stand\n",
      "State : (18, 1, 7), best-pi : stand\n",
      "State : (18, 1, 8), best-pi : stand\n",
      "State : (18, 1, 9), best-pi : stand\n",
      "State : (18, 1, 10), best-pi : stand\n",
      "State : (19, 0, 1), best-pi : stand\n",
      "State : (19, 0, 2), best-pi : stand\n",
      "State : (19, 0, 3), best-pi : stand\n",
      "State : (19, 0, 4), best-pi : stand\n",
      "State : (19, 0, 5), best-pi : stand\n",
      "State : (19, 0, 6), best-pi : stand\n",
      "State : (19, 0, 7), best-pi : stand\n",
      "State : (19, 0, 8), best-pi : stand\n",
      "State : (19, 0, 9), best-pi : stand\n",
      "State : (19, 0, 10), best-pi : stand\n",
      "State : (19, 1, 1), best-pi : stand\n",
      "State : (19, 1, 2), best-pi : stand\n",
      "State : (19, 1, 3), best-pi : stand\n",
      "State : (19, 1, 4), best-pi : stand\n",
      "State : (19, 1, 5), best-pi : stand\n",
      "State : (19, 1, 6), best-pi : stand\n",
      "State : (19, 1, 7), best-pi : stand\n",
      "State : (19, 1, 8), best-pi : stand\n",
      "State : (19, 1, 9), best-pi : stand\n",
      "State : (19, 1, 10), best-pi : hit\n",
      "State : (20, 0, 1), best-pi : stand\n",
      "State : (20, 0, 2), best-pi : stand\n",
      "State : (20, 0, 3), best-pi : stand\n",
      "State : (20, 0, 4), best-pi : stand\n",
      "State : (20, 0, 5), best-pi : stand\n",
      "State : (20, 0, 6), best-pi : stand\n",
      "State : (20, 0, 7), best-pi : stand\n",
      "State : (20, 0, 8), best-pi : stand\n",
      "State : (20, 0, 9), best-pi : stand\n",
      "State : (20, 0, 10), best-pi : stand\n",
      "State : (20, 1, 1), best-pi : stand\n",
      "State : (20, 1, 2), best-pi : stand\n",
      "State : (20, 1, 3), best-pi : stand\n",
      "State : (20, 1, 4), best-pi : stand\n",
      "State : (20, 1, 5), best-pi : stand\n",
      "State : (20, 1, 6), best-pi : stand\n",
      "State : (20, 1, 7), best-pi : stand\n",
      "State : (20, 1, 8), best-pi : stand\n",
      "State : (20, 1, 9), best-pi : stand\n",
      "State : (20, 1, 10), best-pi : stand\n",
      "State : (21, 0, 1), best-pi : stand\n",
      "State : (21, 0, 2), best-pi : stand\n",
      "State : (21, 0, 3), best-pi : stand\n",
      "State : (21, 0, 4), best-pi : stand\n",
      "State : (21, 0, 5), best-pi : stand\n",
      "State : (21, 0, 6), best-pi : stand\n",
      "State : (21, 0, 7), best-pi : stand\n",
      "State : (21, 0, 8), best-pi : stand\n",
      "State : (21, 0, 9), best-pi : stand\n",
      "State : (21, 0, 10), best-pi : stand\n",
      "State : (21, 1, 1), best-pi : stand\n",
      "State : (21, 1, 2), best-pi : stand\n",
      "State : (21, 1, 3), best-pi : stand\n",
      "State : (21, 1, 4), best-pi : stand\n",
      "State : (21, 1, 5), best-pi : stand\n",
      "State : (21, 1, 6), best-pi : stand\n",
      "State : (21, 1, 7), best-pi : stand\n",
      "State : (21, 1, 8), best-pi : stand\n",
      "State : (21, 1, 9), best-pi : stand\n",
      "State : (21, 1, 10), best-pi : stand\n"
     ]
    }
   ],
   "source": [
    "# 最適方策の表示\n",
    "show_policy(S, pi_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 方策オフ型モンテカルロ法\n",
    "シュミレーション時の方策( 挙動方策 ) と本番用の方策 ( 推定方策 ) を別で用意することで, \n",
    "任意の状態行動対 $(s, a)$ が生成される. ただし, 挙動方策は必ずsoftな方策, 推定方策はどちらでも良い. \n",
    "\n",
    "### 3.6.1 方策評価の導出\n",
    "挙動方策 $\\pi_b$ を用いて行動価値関数 $Q^{\\pi_b}$ を求めるが, 欲しいのは推定方策 $\\pi$ の行動価値関数 $ Q^{\\pi} $ である.\n",
    "\n",
    "そこで, $\\pi_b$ でシュミレーションした結果から, $\\pi$ でシュミレーション場合の結果を近似的に求める.\n",
    "\n",
    "方策 $\\pi$ に従うエピソード列 $\\{ (s_t, a_t) \\}_{l \\leqq t \\leqq u}$ が生成される確率は\n",
    "$$ \\begin{eqnarray} \n",
    "    & &P(s_l, a_l, s_{l+1}) \\pi(s_{l+1}, a_{l+1}) P(s_{l+1}, a_{l+1}, s_{l+2}) \\cdots \\pi(s_{u}, a_u) P(s_{u}, a_{u}, s_{u+1}) \\\\\n",
    "    &=& P(s_l, a_l, s_{l+1}) \\prod_{t=l+1}^u \\pi(s_{t+1}, a_{t+1}) P(s_{t+1}, a_{t+1}, s_{t+2}) \n",
    "\\end{eqnarray} $$ \n",
    "方策 $\\pi_b$ に従ったシュミレーションでエピソード列 $\\{ (s_t, a_t) \\}_{l \\leqq t \\leqq u}$ が $N$ 回観測された時, \n",
    "方策$\\pi$ に従うシュミレーションで同エピソード列の観測回数 $M$ は, \n",
    "$$ \\begin{eqnarray}\n",
    "M &=& N \\dfrac{P(s_l, a_l, s_{l+1}) \\prod_{t=l+1}^u \\pi(s_{t+1}, a_{t+1}) P(s_{t+1}, a_{t+1}, s_{t+2})}{P(s_l, a_l, s_{l+1}) \\prod_{t=l+1}^u \\pi_b(s_{t+1}, a_{t+1}) P(s_{t+1}, a_{t+1}, s_{t+2})} \\\\\n",
    "&=& N \\prod_{t = l+1}^u \\dfrac{ \\pi(s_t, a_t) }{ \\pi_b(s_t, a_t) }\n",
    "\\end{eqnarray} $$\n",
    "ちなみに, 推定方策 $\\pi$ をgreedyで更新する場合には, $\\pi(s,a)$ は決定論的な方策となり, 常に1であるから\n",
    "$$ M = N \\prod_{t = l+1}^u \\dfrac{1}{ \\pi_b(s_t, a_t) } $$\n",
    "\n",
    "以前のモンテカルロ法からの変更点を示す.\n",
    "まずわかりやすさのため, $Returns[s][a]$ をリストの合計値 $ReturnsValue[s][a]$ とリストの長さ$ReturnsLength[s][a]$ でもつように変更する.\n",
    "\n",
    "次にある試行のステップ $t$ において, $(s_t, a_t) = (s,a)$ なら\n",
    "$$ \\begin{eqnarray}\n",
    "ReturnsValue[s][a] &\\leftarrow& ReturnsValue[s][a] + R_t \\prod_{t = l+1}^u \\dfrac{ \\pi(s_t, a_t) }{ \\pi_b(s_t, a_t) } \\\\\n",
    "ReturnsLength[s][a] &\\leftarrow& ReturnsLength[s][a] + 1 \\prod_{t = l+1}^u \\dfrac{ \\pi(s_t, a_t) }{ \\pi_b(s_t, a_t) } \n",
    "\\end{eqnarray} $$\n",
    "\n",
    "### 3.6.2 方策改善\n",
    "挙動方策は必ずsoftな方策, 推定方策はどちらでも良い. \n",
    "\n",
    "### 3.6.3 全体のアルゴリズム\n",
    "\n",
    "以下では, 推定方策 $\\pi$ をgreedyで更新しているので, $ \\prod_{t = l+1}^u \\dfrac{1}{ \\pi_b(s_t, a_t) } $ を使っていることに注意. \n",
    "\n",
    "$ReturnsValue$ も 1つ前の $Q$ と $ ReturnsLength$ を使って逆算されているので注意.\n",
    "<img src=\"https://i.stack.imgur.com/Xi0vX.png\" width=500 >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:45:02.554970Z",
     "start_time": "2019-10-04T07:44:39.828084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:22<00:00, 2203.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def greedy(Q):\n",
    "    argmax = np.argmax(Q, axis=1)\n",
    "    new_pi = np.zeros_like(Q)\n",
    "    for s_raw in S:\n",
    "        s = S_to_num[s_raw]\n",
    "        new_pi[s , argmax[s]] = 1.0\n",
    "    return new_pi\n",
    "\n",
    "def epsilon_greedy(Q, epsilon):\n",
    "    # argmaxに 1 - epsilon + epsilon / |A| , それ以外に epsilon / |A|\n",
    "    argmax = np.argmax(Q, axis=1)\n",
    "    new_pi = np.ones_like(Q) * (epsilon / len(A))\n",
    "    for s_raw in S:\n",
    "        s = S_to_num[s_raw]\n",
    "        new_pi[s , argmax[s]] += 1 - epsilon\n",
    "    return new_pi\n",
    "\n",
    "# 初回訪問方策オフ型モンテカルロ法.　\n",
    "# 挙動方策 pi_a : epsilon-greedyで更新. 推定方策 pi : Qからgreedyに更新.\n",
    "def off_policy_Monte_Carlo(epsilon=0.5):\n",
    "    \n",
    "    #############  Initialize  ###################\n",
    "    # Q, を適当に初期化.\n",
    "    Q = np.random.rand(  len(S), len(A) )\n",
    "    # 推定方策をQを元に初期化 (決定論的な方策)\n",
    "    pi = greedy(Q) \n",
    "    # 挙動方策を適当に初期化 ( ソフトな方策 )\n",
    "    pi_b = to_prob(np.random.rand(  len(S), len(A) ))\n",
    "    # Returnsの代わりに, Returnsのリストの長さを持つことにする.\n",
    "    Returns_Length = [ [0]*len(A) for _ in range(len(S)) ]\n",
    "    \n",
    "    ############# シュミレーションをiterations回数やる #################\n",
    "    iterations = 50000\n",
    "    for roop in tqdm(range(iterations)):\n",
    "        # 初期状態の選択\n",
    "        s_0 = np.random.choice(S)\n",
    "        # 終状態から実行しても意味がない\n",
    "        if s_0 == \"PlayerWin\" or s_0 == \"PlayerLose\" or s_0 == \"Draw\":\n",
    "            continue\n",
    "        # 初期行動の選択\n",
    "        a_0 = np.random.choice(A)\n",
    "        \n",
    "        # 挙動方策 pi_b を使って, エピソード生成.\n",
    "        episode = generate_episode(pi_b, s_0, a_0)\n",
    "        \n",
    "        # 既にエピソード列の中に状態行動対( s, a )が登場したかをもつ. Falseで初期化.\n",
    "        visited = np.zeros((len(S), len(A)))\n",
    "        # 収益を計算\n",
    "        R_list = [0] * len (episode)\n",
    "        # pi / pi_b の総積を計算\n",
    "        W_list = [0] * len(episode)\n",
    "        R_t = 0\n",
    "        W_t = 1\n",
    "        # pi[s, a]が0になってしまう場所を記録.\n",
    "        init_t = 0\n",
    "        \n",
    "        # 各時刻における収益とpi/pi_bの総積を前計算.\n",
    "        for i, (s_t, a_t, r_t) in enumerate(reversed(episode)):\n",
    "            s, a = S_to_num[s_t], A_to_num[a_t]\n",
    "            # pi[s,a] == 0.0 なら W_tが以降ずっと0なので終了\n",
    "            if pi[s, a] != 1.0:\n",
    "                init_t = len(episode) - i\n",
    "                break\n",
    "            W_t *= (1 / pi_b[s, a])\n",
    "            R_t += gamma * r_t\n",
    "            R_list[i] = R_t\n",
    "            W_list[i] = W_t\n",
    "        W_list.reverse()\n",
    "        R_list.reverse()\n",
    "        \n",
    "        ########## 方策評価　############\n",
    "        # 先頭からエピソードをみる ( 初回モンテカルロ法 )\n",
    "        for i, (s_t, a_t, r_t) in enumerate(episode[init_t:]):\n",
    "            # 状態と行動を数字に変換\n",
    "            s, a = S_to_num[s_t], A_to_num[a_t]\n",
    "            idx = init_t + i\n",
    "            # ステップtでの pi / pi_b の総積\n",
    "            W_t = W_list[idx]\n",
    "            # ステップtでの収益\n",
    "            R_t = R_list[idx]\n",
    "            # 初回訪問の場合のみ扱う.\n",
    "            if not visited[s,a]:\n",
    "                Returns_Length[s][a] += 1 * W_t\n",
    "                # 次の式と同値 :\n",
    "                # Q[s,a] = (Q[s,a] * (Returns_Length[s][a] - 1*W_t) + R_t*W_t) / Returns_Length[s,a]\n",
    "                Q[s,a] = Q[s,a] + ((R_t - Q[s,a])*W_t) / (Returns_Length[s][a])\n",
    "                visited[s, a] = True\n",
    "        \n",
    "       ########## 方策改善　############\n",
    "        # 推定方策の更新.\n",
    "        pi = greedy(Q)\n",
    "        # 行動方策の更新 \n",
    "        pi_b = epsilon_greedy(Q, epsilon)\n",
    "        # epsilonを小さく.\n",
    "        epsilon /= 2\n",
    "        \n",
    "    return Q, pi, Returns_Length\n",
    "\n",
    "Q, pi_off, Returns_Length = off_policy_Monte_Carlo(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:45:02.570045Z",
     "start_time": "2019-10-04T07:45:02.557702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State : (12, 0, 1), best-pi : hit\n",
      "State : (12, 0, 2), best-pi : hit\n",
      "State : (12, 0, 3), best-pi : hit\n",
      "State : (12, 0, 4), best-pi : stand\n",
      "State : (12, 0, 5), best-pi : stand\n",
      "State : (12, 0, 6), best-pi : hit\n",
      "State : (12, 0, 7), best-pi : stand\n",
      "State : (12, 0, 8), best-pi : hit\n",
      "State : (12, 0, 9), best-pi : hit\n",
      "State : (12, 0, 10), best-pi : hit\n",
      "State : (12, 1, 1), best-pi : hit\n",
      "State : (12, 1, 2), best-pi : stand\n",
      "State : (12, 1, 3), best-pi : stand\n",
      "State : (12, 1, 4), best-pi : stand\n",
      "State : (12, 1, 5), best-pi : hit\n",
      "State : (12, 1, 6), best-pi : hit\n",
      "State : (12, 1, 7), best-pi : stand\n",
      "State : (12, 1, 8), best-pi : hit\n",
      "State : (12, 1, 9), best-pi : hit\n",
      "State : (12, 1, 10), best-pi : hit\n",
      "State : (13, 0, 1), best-pi : hit\n",
      "State : (13, 0, 2), best-pi : stand\n",
      "State : (13, 0, 3), best-pi : hit\n",
      "State : (13, 0, 4), best-pi : hit\n",
      "State : (13, 0, 5), best-pi : hit\n",
      "State : (13, 0, 6), best-pi : stand\n",
      "State : (13, 0, 7), best-pi : hit\n",
      "State : (13, 0, 8), best-pi : stand\n",
      "State : (13, 0, 9), best-pi : hit\n",
      "State : (13, 0, 10), best-pi : stand\n",
      "State : (13, 1, 1), best-pi : hit\n",
      "State : (13, 1, 2), best-pi : stand\n",
      "State : (13, 1, 3), best-pi : stand\n",
      "State : (13, 1, 4), best-pi : hit\n",
      "State : (13, 1, 5), best-pi : hit\n",
      "State : (13, 1, 6), best-pi : hit\n",
      "State : (13, 1, 7), best-pi : hit\n",
      "State : (13, 1, 8), best-pi : hit\n",
      "State : (13, 1, 9), best-pi : hit\n",
      "State : (13, 1, 10), best-pi : hit\n",
      "State : (14, 0, 1), best-pi : hit\n",
      "State : (14, 0, 2), best-pi : hit\n",
      "State : (14, 0, 3), best-pi : stand\n",
      "State : (14, 0, 4), best-pi : hit\n",
      "State : (14, 0, 5), best-pi : hit\n",
      "State : (14, 0, 6), best-pi : stand\n",
      "State : (14, 0, 7), best-pi : hit\n",
      "State : (14, 0, 8), best-pi : hit\n",
      "State : (14, 0, 9), best-pi : stand\n",
      "State : (14, 0, 10), best-pi : stand\n",
      "State : (14, 1, 1), best-pi : hit\n",
      "State : (14, 1, 2), best-pi : hit\n",
      "State : (14, 1, 3), best-pi : hit\n",
      "State : (14, 1, 4), best-pi : hit\n",
      "State : (14, 1, 5), best-pi : stand\n",
      "State : (14, 1, 6), best-pi : hit\n",
      "State : (14, 1, 7), best-pi : hit\n",
      "State : (14, 1, 8), best-pi : hit\n",
      "State : (14, 1, 9), best-pi : hit\n",
      "State : (14, 1, 10), best-pi : hit\n",
      "State : (15, 0, 1), best-pi : hit\n",
      "State : (15, 0, 2), best-pi : hit\n",
      "State : (15, 0, 3), best-pi : hit\n",
      "State : (15, 0, 4), best-pi : hit\n",
      "State : (15, 0, 5), best-pi : hit\n",
      "State : (15, 0, 6), best-pi : hit\n",
      "State : (15, 0, 7), best-pi : hit\n",
      "State : (15, 0, 8), best-pi : stand\n",
      "State : (15, 0, 9), best-pi : hit\n",
      "State : (15, 0, 10), best-pi : stand\n",
      "State : (15, 1, 1), best-pi : stand\n",
      "State : (15, 1, 2), best-pi : hit\n",
      "State : (15, 1, 3), best-pi : hit\n",
      "State : (15, 1, 4), best-pi : hit\n",
      "State : (15, 1, 5), best-pi : hit\n",
      "State : (15, 1, 6), best-pi : stand\n",
      "State : (15, 1, 7), best-pi : hit\n",
      "State : (15, 1, 8), best-pi : hit\n",
      "State : (15, 1, 9), best-pi : hit\n",
      "State : (15, 1, 10), best-pi : stand\n",
      "State : (16, 0, 1), best-pi : hit\n",
      "State : (16, 0, 2), best-pi : hit\n",
      "State : (16, 0, 3), best-pi : stand\n",
      "State : (16, 0, 4), best-pi : stand\n",
      "State : (16, 0, 5), best-pi : stand\n",
      "State : (16, 0, 6), best-pi : stand\n",
      "State : (16, 0, 7), best-pi : stand\n",
      "State : (16, 0, 8), best-pi : hit\n",
      "State : (16, 0, 9), best-pi : hit\n",
      "State : (16, 0, 10), best-pi : stand\n",
      "State : (16, 1, 1), best-pi : hit\n",
      "State : (16, 1, 2), best-pi : hit\n",
      "State : (16, 1, 3), best-pi : hit\n",
      "State : (16, 1, 4), best-pi : hit\n",
      "State : (16, 1, 5), best-pi : hit\n",
      "State : (16, 1, 6), best-pi : hit\n",
      "State : (16, 1, 7), best-pi : hit\n",
      "State : (16, 1, 8), best-pi : hit\n",
      "State : (16, 1, 9), best-pi : hit\n",
      "State : (16, 1, 10), best-pi : hit\n",
      "State : (17, 0, 1), best-pi : hit\n",
      "State : (17, 0, 2), best-pi : hit\n",
      "State : (17, 0, 3), best-pi : stand\n",
      "State : (17, 0, 4), best-pi : stand\n",
      "State : (17, 0, 5), best-pi : stand\n",
      "State : (17, 0, 6), best-pi : stand\n",
      "State : (17, 0, 7), best-pi : stand\n",
      "State : (17, 0, 8), best-pi : hit\n",
      "State : (17, 0, 9), best-pi : stand\n",
      "State : (17, 0, 10), best-pi : hit\n",
      "State : (17, 1, 1), best-pi : hit\n",
      "State : (17, 1, 2), best-pi : hit\n",
      "State : (17, 1, 3), best-pi : hit\n",
      "State : (17, 1, 4), best-pi : stand\n",
      "State : (17, 1, 5), best-pi : stand\n",
      "State : (17, 1, 6), best-pi : stand\n",
      "State : (17, 1, 7), best-pi : stand\n",
      "State : (17, 1, 8), best-pi : hit\n",
      "State : (17, 1, 9), best-pi : hit\n",
      "State : (17, 1, 10), best-pi : stand\n",
      "State : (18, 0, 1), best-pi : hit\n",
      "State : (18, 0, 2), best-pi : stand\n",
      "State : (18, 0, 3), best-pi : stand\n",
      "State : (18, 0, 4), best-pi : hit\n",
      "State : (18, 0, 5), best-pi : stand\n",
      "State : (18, 0, 6), best-pi : stand\n",
      "State : (18, 0, 7), best-pi : stand\n",
      "State : (18, 0, 8), best-pi : stand\n",
      "State : (18, 0, 9), best-pi : stand\n",
      "State : (18, 0, 10), best-pi : hit\n",
      "State : (18, 1, 1), best-pi : hit\n",
      "State : (18, 1, 2), best-pi : stand\n",
      "State : (18, 1, 3), best-pi : stand\n",
      "State : (18, 1, 4), best-pi : hit\n",
      "State : (18, 1, 5), best-pi : hit\n",
      "State : (18, 1, 6), best-pi : stand\n",
      "State : (18, 1, 7), best-pi : stand\n",
      "State : (18, 1, 8), best-pi : stand\n",
      "State : (18, 1, 9), best-pi : stand\n",
      "State : (18, 1, 10), best-pi : hit\n",
      "State : (19, 0, 1), best-pi : hit\n",
      "State : (19, 0, 2), best-pi : stand\n",
      "State : (19, 0, 3), best-pi : stand\n",
      "State : (19, 0, 4), best-pi : hit\n",
      "State : (19, 0, 5), best-pi : stand\n",
      "State : (19, 0, 6), best-pi : stand\n",
      "State : (19, 0, 7), best-pi : stand\n",
      "State : (19, 0, 8), best-pi : stand\n",
      "State : (19, 0, 9), best-pi : stand\n",
      "State : (19, 0, 10), best-pi : hit\n",
      "State : (19, 1, 1), best-pi : hit\n",
      "State : (19, 1, 2), best-pi : stand\n",
      "State : (19, 1, 3), best-pi : stand\n",
      "State : (19, 1, 4), best-pi : stand\n",
      "State : (19, 1, 5), best-pi : stand\n",
      "State : (19, 1, 6), best-pi : stand\n",
      "State : (19, 1, 7), best-pi : stand\n",
      "State : (19, 1, 8), best-pi : stand\n",
      "State : (19, 1, 9), best-pi : stand\n",
      "State : (19, 1, 10), best-pi : hit\n",
      "State : (20, 0, 1), best-pi : hit\n",
      "State : (20, 0, 2), best-pi : stand\n",
      "State : (20, 0, 3), best-pi : stand\n",
      "State : (20, 0, 4), best-pi : stand\n",
      "State : (20, 0, 5), best-pi : stand\n",
      "State : (20, 0, 6), best-pi : stand\n",
      "State : (20, 0, 7), best-pi : hit\n",
      "State : (20, 0, 8), best-pi : stand\n",
      "State : (20, 0, 9), best-pi : stand\n",
      "State : (20, 0, 10), best-pi : stand\n",
      "State : (20, 1, 1), best-pi : hit\n",
      "State : (20, 1, 2), best-pi : stand\n",
      "State : (20, 1, 3), best-pi : stand\n",
      "State : (20, 1, 4), best-pi : stand\n",
      "State : (20, 1, 5), best-pi : stand\n",
      "State : (20, 1, 6), best-pi : stand\n",
      "State : (20, 1, 7), best-pi : stand\n",
      "State : (20, 1, 8), best-pi : stand\n",
      "State : (20, 1, 9), best-pi : stand\n",
      "State : (20, 1, 10), best-pi : stand\n",
      "State : (21, 0, 1), best-pi : stand\n",
      "State : (21, 0, 2), best-pi : stand\n",
      "State : (21, 0, 3), best-pi : stand\n",
      "State : (21, 0, 4), best-pi : stand\n",
      "State : (21, 0, 5), best-pi : stand\n",
      "State : (21, 0, 6), best-pi : stand\n",
      "State : (21, 0, 7), best-pi : stand\n",
      "State : (21, 0, 8), best-pi : stand\n",
      "State : (21, 0, 9), best-pi : stand\n",
      "State : (21, 0, 10), best-pi : stand\n",
      "State : (21, 1, 1), best-pi : stand\n",
      "State : (21, 1, 2), best-pi : stand\n",
      "State : (21, 1, 3), best-pi : stand\n",
      "State : (21, 1, 4), best-pi : stand\n",
      "State : (21, 1, 5), best-pi : stand\n",
      "State : (21, 1, 6), best-pi : stand\n",
      "State : (21, 1, 7), best-pi : stand\n",
      "State : (21, 1, 8), best-pi : stand\n",
      "State : (21, 1, 9), best-pi : stand\n",
      "State : (21, 1, 10), best-pi : stand\n"
     ]
    }
   ],
   "source": [
    "show_policy(S, pi_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
