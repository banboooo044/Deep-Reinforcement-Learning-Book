{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:24.044181Z",
     "start_time": "2019-09-24T05:15:23.599102Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用するパッケージの宣言\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.530071Z",
     "start_time": "2019-09-24T05:15:24.050888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAElCAYAAABect+9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10488fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初期位置での迷路の様子\n",
    "\n",
    "# 図を描く大きさと、図の変数名を宣言\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "# 赤い壁を描く\n",
    "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
    "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
    "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
    "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
    "\n",
    "# 状態を示す文字S0～S8を描く\n",
    "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
    "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
    "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
    "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
    "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
    "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
    "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
    "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
    "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
    "plt.text(0.5, 2.3, 'START', ha='center')\n",
    "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
    "\n",
    "# 描画範囲の設定と目盛りを消す設定\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 3)\n",
    "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
    "                labelbottom='off', right='off', left='off', labelleft='off')\n",
    "\n",
    "# 現在地S0に緑丸を描画する\n",
    "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言葉の定義\n",
    "## 1. マルコフ決定過程 $( S, A, P, r, \\pi, \\gamma)$\n",
    "* 状態集合 $S$\n",
    "* 行動集合 $A$\n",
    "* 遷移確率 $P : S \\times A \\times S \\mapsto [0, 1]$ <br>\n",
    "$ P(s,\\ a,\\ s') = $ 状態 $s$ で行動 $a$ をとった時に状態 $s'$ に遷移する確率\n",
    "* 報酬関数 $r : S \\times A \\times S \\mapsto \\mathbb{R}$ <br>\n",
    "$ r(s,\\ a,\\ s') = $ 状態 $s$で行動 $a$ をとり, 状態 $s'$ に遷移した時にもらえる報酬.\n",
    "* 方策(政策) $ \\pi : S \\times A \\mapsto [0, 1]$ <br>\n",
    "$ \\pi (s,\\ a) = $ 状態 $s$ で行動 $a$ をとる確率.\n",
    "* 割引率 $\\gamma,\\ 0 < \\gamma < 1$\n",
    "\n",
    "迷路ゲームの例\n",
    "* $S = \\{ S0,\\ S1, \\dots,\\ S8 \\}$\n",
    "* $A = \\{ \\text{上},\\ \\text{下},\\ \\text{右},\\ \\text{左} \\}$\n",
    "* $P(S0,\\ \\text{右},\\ S1) = 1,\\ P(S0,\\ \\text{右},\\ S2) = 0, \\dots $ <br>\n",
    "迷路ゲームにおいては, 行動に対して次の状態が一意に決まる.\n",
    "* $r(S7, \\text{右}, S8) = 1.0,\\ r(S0, \\text{右}, S1) = 0.0,\\ \\dots $ <br>\n",
    "ゴールについた時に報酬1.0が貰えると定める.それ以外は報酬0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迷路ゲーム\n",
    "最適な方策(政策) $ \\pi : S \\times A \\mapsto [0, 1]$ を求めたい.\n",
    "\n",
    "つまり, \n",
    "$$\\pi(S0, \\text{下}) \\simeq 1,\\ \\pi(S3, \\text{右})  \\simeq 1, \\pi(S4, \\text{下})  \\simeq 1,\\ \\pi(S7, \\text{右})  \\simeq 1$$\n",
    "となって4手(最適)でゴールできるように$\\pi$を学習したい.\n",
    "\n",
    "今回は **方策勾配法** を使って$\\pi$の学習を行う.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方策勾配法\n",
    "\n",
    "### 1. 方策関数を $\\boldsymbol{\\theta}$ を用いて定義\n",
    "$\\pi$を媒介変数 $\\boldsymbol{\\theta} = ( \\theta_{(S0, \\text{上})},\\ \\theta_{(S0, \\text{下})},\\ ,\\ \\dots \\theta_{(S1, \\text{上})},\\ \\dots,\\ \\theta_{(S7, \\text{右})} ) $ を用いて表すことにする.\n",
    "\n",
    "$$\\pi( s_i,\\ a_j) = \\dfrac{ \\exp{ \\beta \\theta_{( s_i,\\ a_j)}} } { \\sum_{ k } \\exp { \\beta \\theta_{( s_i,\\ a_k)}} }$$\n",
    "\n",
    "例 :\n",
    "$$ \\pi( S0,\\ \\text{下} ) = \\dfrac{ \\exp{ \\beta \\theta_{( S0,\\ \\text{下})}} } { \\exp{  \\beta \\theta_{( S0,\\ \\text{下})}} +\\exp{ \\beta \\theta_{( S0,\\ \\text{上})}} + \\exp{ \\beta  \\theta_{( S0,\\ \\text{左})}} + \\exp{ \\beta \\theta_{( S0,\\ \\text{右})}} } $$\n",
    "\n",
    "$\\beta$は逆温度定数. 温度パラメータをつけることの意味は以下を参照.\n",
    "https://qiita.com/nkriskeeic/items/db3b4b5e835e63a7f243\n",
    "\n",
    "またプログラムでは, $\\boldsymbol{\\theta}$をnumpy行列で表しているけど数学上は <font color=\"Red\">ベクトル</font>\n",
    "として考えるので注意!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.566969Z",
     "start_time": "2019-09-24T05:15:25.537050Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初期の方策を決定するパラメータtheta_0を設定\n",
    "\n",
    "# 行は状態0～7、列は移動方向で↑、→、↓、←を表す\n",
    "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
    "                    [np.nan, 1, np.nan, 1],  # s1\n",
    "                    [np.nan, np.nan, 1, 1],  # s2\n",
    "                    [1, 1, 1, np.nan],  # s3\n",
    "                    [np.nan, np.nan, 1, 1],  # s4\n",
    "                    [1, np.nan, np.nan, np.nan],  # s5\n",
    "                    [1, np.nan, np.nan, np.nan],  # s6\n",
    "                    [1, 1, np.nan, np.nan],  # s7、※s8はゴールなので、方策はなし\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.644196Z",
     "start_time": "2019-09-24T05:15:25.574768Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方策パラメータthetaを行動方策piにソフトマックス関数で変換する手法の定義\n",
    "\n",
    "\n",
    "def softmax_convert_into_pi_from_theta(theta):\n",
    "    '''ソフトマックス関数で割合を計算する'''\n",
    "\n",
    "    beta = 1.0\n",
    "    [m, n] = theta.shape  # thetaの行列サイズを取得\n",
    "    pi = np.zeros((m, n))\n",
    "\n",
    "    exp_theta = np.exp(beta * theta)  # thetaをexp(theta)へと変換\n",
    "\n",
    "    for i in range(0, m):\n",
    "        # pi[i, :] = theta[i, :] / np.nansum(theta[i, :])\n",
    "        # simpleに割合の計算の場合\n",
    "\n",
    "        pi[i, :] = exp_theta[i, :] / np.nansum(exp_theta[i, :])\n",
    "        # softmaxで計算の場合\n",
    "\n",
    "    pi = np.nan_to_num(pi)  # nanを0に変換\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.686981Z",
     "start_time": "2019-09-24T05:15:25.656718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.5         0.5         0.        ]\n",
      " [ 0.          0.5         0.          0.5       ]\n",
      " [ 0.          0.          0.5         0.5       ]\n",
      " [ 0.33333333  0.33333333  0.33333333  0.        ]\n",
      " [ 0.          0.          0.5         0.5       ]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [ 0.5         0.5         0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 初期の方策pi_0を求める\n",
    "pi_0 = softmax_convert_into_pi_from_theta(theta_0)\n",
    "print(pi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.718829Z",
     "start_time": "2019-09-24T05:15:25.693847Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 行動aと1step移動後の状態sを求める関数を定義\n",
    "\n",
    "\n",
    "def get_action_and_next_s(pi, s):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    # pi[s,:]の確率に従って、directionが選択される\n",
    "    next_direction = np.random.choice(direction, p=pi[s, :])\n",
    "\n",
    "    if next_direction == \"up\":\n",
    "        action = 0\n",
    "        s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n",
    "    elif next_direction == \"right\":\n",
    "        action = 1\n",
    "        s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
    "    elif next_direction == \"down\":\n",
    "        action = 2\n",
    "        s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n",
    "    elif next_direction == \"left\":\n",
    "        action = 3\n",
    "        s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
    "\n",
    "    return [action, s_next]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.746807Z",
     "start_time": "2019-09-24T05:15:25.728139Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 迷路を解く関数の定義、状態と行動の履歴を出力\n",
    "\n",
    "\n",
    "def goal_maze_ret_s_a(pi):\n",
    "    s = 0  # スタート地点\n",
    "    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
    "\n",
    "    while (1):  # ゴールするまでループ\n",
    "        [action, next_s] = get_action_and_next_s(pi, s)\n",
    "        s_a_history[-1][1] = action\n",
    "        # 現在の状態（つまり一番最後なのでindex=-1）の行動を代入\n",
    "\n",
    "        s_a_history.append([next_s, np.nan])\n",
    "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
    "\n",
    "        if next_s == 8:  # ゴール地点なら終了\n",
    "            break\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return s_a_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.759209Z",
     "start_time": "2019-09-24T05:15:25.752336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2], [3, 0], [0, 1], [1, 3], [0, 2], [3, 2], [6, 0], [3, 2], [6, 0], [3, 1], [4, 2], [7, 0], [4, 3], [3, 2], [6, 0], [3, 1], [4, 2], [7, 0], [4, 2], [7, 1], [8, nan]]\n",
      "迷路を解くのにかかったステップ数は20です\n"
     ]
    }
   ],
   "source": [
    "# 初期の方策で迷路を解く\n",
    "s_a_history = goal_maze_ret_s_a(pi_0)\n",
    "print(s_a_history)\n",
    "print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方策勾配法\n",
    "\n",
    "### 2. 更新式の導出\n",
    "\n",
    "方策 $\\pi$ の良さを表す関数 $ J( \\boldsymbol{\\theta} ) : \\mathbb{R}^{ |S| \\cdot |A| } \\mapsto \\mathbb{R}$ を考える.\n",
    "\n",
    "この時, $J( \\boldsymbol{\\theta} )$ を最大化するような $\\boldsymbol{\\theta}$ を最急降下法によって求める.\n",
    "\n",
    "$$  \\boldsymbol{\\theta}^{(k+1)} \\leftarrow \\boldsymbol{\\theta}^{(k)} + \\eta \\nabla J( \\boldsymbol{\\theta} )  \\quad ( \\eta : \\text{学習率} )$$\n",
    "\n",
    "最大化したいので, $+$になっていることに注意!!\n",
    "\n",
    "#### 2.1 $ J( \\boldsymbol{\\theta} )$ の定義\n",
    "1. 平均報酬による定義\n",
    "$$ J( \\boldsymbol{\\theta} ) = \\lim_{n \\to \\infty} \\dfrac{1}{n} \\mathbb{E}[ \\sum_{t=1}^n r_t ] $$\n",
    "2. 割引報酬による定義\n",
    "$$  J( \\boldsymbol{\\theta} ) = \\lim_{n \\to \\infty} \\mathbb{E}[ \\sum_{t=1}^n \\gamma^{t-1} r_t  ; s_0 ] $$\n",
    "\n",
    "#### 2.2 方策勾配定理\n",
    "式変形すると, どちらの定義でも以下のようになる.( 式変形参考 : [book](https://github.com/komi1230/Resume/tree/master/book_reinforcement) p52あたり )\n",
    "$$ \\nabla  J( \\boldsymbol{\\theta} ) = \\mathbb{E}[ Q^{\\pi}(s, a) \\nabla \\log \\pi(s,a) ] \\quad ( \\text{方策勾配定理} ) $$ \n",
    "\n",
    "#### 2.3 REINFORCE アルゴリズム\n",
    "期待値をサンプリングによって近似する. ( $M$ : 試行回数, $T$ : ステップ数, $M$は十分大きいと仮定. )\n",
    "$$ \\nabla  J( \\boldsymbol{\\theta} ) \\simeq \\dfrac{1}{M} \\sum_{m=1}^{M} \\dfrac{1}{T} \\sum_{t=1}^{T} Q^{\\pi}(s_t, a_t) \\nabla \\log \\pi(s_t,a_t)$$\n",
    "\n",
    "さらに$Q^{\\pi}(s, a)$が未知なので, 収益$R_t$で近似する.\n",
    "$$ Q^{\\pi}(s,a) \\simeq R_t = \n",
    "\\begin{cases} \\sum_{k=1}^{T} r_{t+k} \\ ( \\text{有限エピソードの時はこちらでも良い.} ) \\\\ \n",
    "\\sum_{k=1}^{\\infty} \\gamma^{k-1} r_{t+k} \n",
    "\\end{cases} $$\n",
    "\n",
    "( 参考 : [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) )\n",
    "\n",
    "ベースメントを用いて分散を小さくする等の工夫もあるらしい.\n",
    "\n",
    "最終的に, \n",
    "$$\n",
    "\\nabla  J( \\boldsymbol{\\theta} ) \\simeq \\dfrac{1}{M} \\sum_{m=1}^{M} \\dfrac{1}{T} \\sum_{t=1}^{T} R_t \\nabla \\log \\pi(s_t,a_t) \n",
    "$$\n",
    "\n",
    "#### 2.4 迷路ゲームの場合の方策勾配法.\n",
    "ゴールに着いた時にのみ報酬1がもらえるとする.つまり, \n",
    "$r_t = \\begin{cases} 1 \\ ( t = T ) \\\\ 0 \\ ( otherwise ) \\end{cases}$\n",
    "であり, \n",
    "$$R_t = \\sum_{k=1}^{T} r_{t+k} = 1$$\n",
    "\n",
    "また, 1エピソードを得るごとに更新しているので, $M = 1$として良い.\n",
    "\n",
    "方策関数はsoftmax関数で定義していた. \n",
    "$$ \\pi( s_i,\\ a_j) = \\dfrac{ \\exp{ \\beta \\theta_{( s_i,\\ a_j)}} } { \\sum_{ k } \\exp { \\beta \\theta_{( s_i,\\ a_k)}} }$$\n",
    "\n",
    "これらを使って式を変形すると\n",
    "$$ \\nabla  J( \\boldsymbol{\\theta} ) = \n",
    "( \\dfrac{ \\partial  J( \\boldsymbol{\\theta} )}{ \\partial \\theta_{( S0,\\ \\text{下})}},\\ \n",
    "\\dfrac{ \\partial  J( \\boldsymbol{\\theta} )}{ \\partial \\theta_{( S0,\\ \\text{上})}},\\ \\dots,\\ \n",
    "\\dfrac{ \\partial  J( \\boldsymbol{\\theta} )}{ \\partial \\theta_{( S7,\\ \\text{右})}} ) $$\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "\\dfrac{ \\partial  J( \\boldsymbol{\\theta} )}{ \\partial \\theta_{(s_i, a_j)}} &\\simeq&\n",
    "\\dfrac{1}{T} \\sum_{t=1}^T \\dfrac{ \\partial}{ \\partial \\theta_{(s_i, a_j)}} ( \\log \\exp{\\beta \\theta_{(s_t, a_t)}} - \\log \\sum_{k=1}^4 \\exp{\\beta \\theta_{(s_t, a_k)}} )  \\\\\n",
    "&=& \\dfrac{1}{T} ( \\sum_{s_t = s_i,\\ a_t = a_j} \\beta \n",
    "- \\sum_{s_t = s_i} \\dfrac{\\beta \\exp{\\beta \\theta_{(s_i, a_j)}}}{\\sum_{k=1}^4 \\exp{\\beta \\theta_{(s_i, a_k)}}} )\\\\\n",
    "&=& \\dfrac{\\beta}{T} ( N( s_t = s_i, a_t = a_j) - N(s_t = s_i) \\pi(s_i, a_j) )\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$\\beta = 1.0$より, 書籍の式が求まった.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.770948Z",
     "start_time": "2019-09-24T05:15:25.761400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thetaの更新関数を定義します\n",
    "\n",
    "def update_theta(theta, pi, s_a_history):\n",
    "    eta = 0.1 # 学習率\n",
    "    T = len(s_a_history) - 1  # ゴールまでの総ステップ数\n",
    "\n",
    "    [m, n] = theta.shape  # thetaの行列サイズを取得\n",
    "    delta_theta = theta.copy()  # Δthetaの元を作成、ポインタ参照なので、delta_theta = thetaはダメ\n",
    "\n",
    "    # delta_thetaを要素ごとに求めます\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n):\n",
    "            if not(np.isnan(theta[i, j])):  # thetaがnanでない場合\n",
    "\n",
    "                SA_i = [SA for SA in s_a_history if SA[0] == i]\n",
    "                # 履歴から状態iのものを取り出すリスト内包表記です\n",
    "\n",
    "                SA_ij = [SA for SA in s_a_history if SA == [i, j]]\n",
    "                # 状態iで行動jをしたものを取り出す\n",
    "\n",
    "                N_i = len(SA_i)  # 状態iで行動した総回数\n",
    "                N_ij = len(SA_ij)  # 状態iで行動jをとった回数\n",
    "                \n",
    "                # 初版では符号の正負に間違いがありました（修正日：180703）\n",
    "                #delta_theta[i, j] = (N_ij + pi[i, j] * N_i) / T\n",
    "                delta_theta[i, j] = (N_ij - pi[i, j] * N_i) / T\n",
    "\n",
    "    new_theta = theta + eta * delta_theta\n",
    "\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:25.788779Z",
     "start_time": "2019-09-24T05:15:25.772563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.49875     0.50125     0.        ]\n",
      " [ 0.          0.49875     0.          0.50125   ]\n",
      " [ 0.          0.          0.5         0.5       ]\n",
      " [ 0.33166806  0.33333056  0.33500138  0.        ]\n",
      " [ 0.          0.          0.50249998  0.49750002]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [ 0.50125     0.49875     0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 方策の更新\n",
    "new_theta = update_theta(theta_0, pi_0, s_a_history)\n",
    "pi = softmax_convert_into_pi_from_theta(new_theta)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:34.062363Z",
     "start_time": "2019-09-24T05:15:25.794339Z"
    }
   },
   "outputs": [],
   "source": [
    "# 方策勾配法で迷路を解く\n",
    "\n",
    "# 初版で、def update_thetaに間違いがあった関係で、終了条件を変更します（修正日：180703）\n",
    "#stop_epsilon = 10**-8  # 10^-8よりも方策に変化が少なくなったら学習終了とする\n",
    "stop_epsilon = 10**-4  # 10^-4よりも方策に変化が少なくなったら学習終了とする\n",
    "\n",
    "\n",
    "theta = theta_0\n",
    "pi = pi_0\n",
    "\n",
    "is_continue = True\n",
    "count = 1\n",
    "while is_continue:  # is_continueがFalseになるまで繰り返す\n",
    "    s_a_history = goal_maze_ret_s_a(pi)  # 方策πで迷路内を探索した履歴を求める\n",
    "    new_theta = update_theta(theta, pi, s_a_history)  # パラメータΘを更新\n",
    "    new_pi = softmax_convert_into_pi_from_theta(new_theta)  # 方策πの更新\n",
    "\n",
    "    #print(np.sum(np.abs(new_pi - pi)))  # 方策の変化を出力\n",
    "    #print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
    "\n",
    "    if np.sum(np.abs(new_pi - pi)) < stop_epsilon:\n",
    "        is_continue = False\n",
    "    else:\n",
    "        theta = new_theta\n",
    "        pi = new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:34.101079Z",
     "start_time": "2019-09-24T05:15:34.079237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     0.014  0.986  0.   ]\n",
      " [ 0.     0.252  0.     0.748]\n",
      " [ 0.     0.     0.438  0.562]\n",
      " [ 0.012  0.979  0.01   0.   ]\n",
      " [ 0.     0.     0.984  0.016]\n",
      " [ 1.     0.     0.     0.   ]\n",
      " [ 1.     0.     0.     0.   ]\n",
      " [ 0.014  0.986  0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# 最終的な方策を確認\n",
    "np.set_printoptions(precision=3, suppress=True)  # 有効桁数3、指数表示しないという設定\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T05:15:34.896348Z",
     "start_time": "2019-09-24T05:15:34.103858Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FuncAnimation' object has no attribute 'to_jshtml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eba6f551c60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     s_a_history), interval=200, repeat=False)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jshtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FuncAnimation' object has no attribute 'to_jshtml'"
     ]
    }
   ],
   "source": [
    "# エージェントの移動の様子を可視化します\n",
    "# 参考URL http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def init():\n",
    "    # 背景画像の初期化\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    # フレームごとの描画内容\n",
    "    state = s_a_history[i][0]  # 現在の場所を描く\n",
    "    x = (state % 3) + 0.5  # 状態のx座標は、3で割った余り+0.5\n",
    "    y = 2.5 - int(state / 3)  # y座標は3で割った商を2.5から引く\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "#　初期化関数とフレームごとの描画関数を用いて動画を作成\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(\n",
    "    s_a_history), interval=200, repeat=False)\n",
    "\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
